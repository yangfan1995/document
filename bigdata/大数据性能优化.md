# 大数据性能优化

## 优化要点

 ![优化要点](E:\document\img\性能优化\Hive优化要点.png)

 ![“map reduce”的图片搜索结果](https://www.talend.com/wp-content/uploads/what-is-mapreduce.jpg) 

## Hive优化

### 表分区优化

##### 优化场景

针对某些数据表，为了调高检索速度进行Hive表分区处理。

##### 优化条件

1. 查询维度、业务需求
2. 日期分区
3. 类型分区

 ##### 优化原理

将列值作为目录来存放数据，就是一个分区。这样查询时使用分区列进行过滤，只需根据列值直接扫描对应目录下的数据，不扫描其他不关心的分区，快速定位，提高查询效率。 

##### 优化方式

1. 分区查询和未分区查询

   ```sql
   select * from app_da_veh_log where dt='20190611' and rece_time='20190611082341';
   ```

   ![1571197873734](https://s2.ax1x.com/2019/10/16/KFooCj.md.png)

   ```sql
   select * from app_da_veh_log where rece_time='20190611082341';
   ```

   ![1571197926347](https://s2.ax1x.com/2019/10/16/KFo55Q.md.png)

##### 注意事项

1. 分区字段是否太过紧凑，Hive以文件夹分区，太过紧凑会导致文件夹数量过多影响性能。
2. 动态分区时，每个分区一个reducer，分区过多reducer过多。

### 压缩优化

1. input（Map）:
   + 大数据从HDFS读取数据
   + 如果你的文件很大，那么压缩将减少磁盘读取的消耗
   + 使用类似bzip2和lzo这种能够split的算法进行压缩（如果不能split那么只有一个task处理，可以split的方法能使用多task并行处理）
2. temp:中间转换
   + Map的输出是写入磁盘，并且通过网络传输
   + 使用compression目的就是为了减少写磁盘和网络传输的负载
   + 即使输入和输出没有进行压缩，只在中间使用了压缩，也能带来性能的提升
   + 使用快的压缩方式编码，如snappy、lzo，选用压缩快的方式是因为后续程序在等待数据进行后续处理，如果采用更高压缩比的方式，那么压缩时间和解压时间将会拖慢整个程序的执行时间
3. output（reducer）:
   + mapreduce的输出通常是用来归档，或者作为下一个作业的输入（需要考虑能否分片）
   + 如果是归档，那么要使用高的压缩比来进行压缩，减少归档的空间
   + 如果是归档，那么要使用能够split的压缩方式进行压缩，一边提高下一次作业的并行度
| 压缩格式 | codec类     | 算法    | 扩展名  | 多文件 | splitable | native | 工具  | hadoop自带 | 压缩速度 | 压缩比率 |
| -------- | ----------- | ------- | ------- | ------ | --------- | ------ | ----- | ---------- | -------- | -------- |
| gzip     | GzipCodec   | deflate | .gz     | 否     | 否        | 是     | gzip  | 是         | 中       | 中       |
| bzip2    | Bzip2Codec  | bzip2   | .bz2    | 是     | 是        | 否     | bzip2 | 是         | 慢       | 小       |
| lzo、Lz4 | LzopCodec   | lzo     | .lzo    | 否     | 是        | 是     | lzop  | 否         | 快       | 大       |
| snappy   | SnappyCodec | snappy  | .snappy | 否     | 否        | 是     | 无    | 否         | 快       | 大       |

+ org.apache.hadoop.io.compress.DefaultCodec
+ org.apache.hadoop.io.compress.GzipCodec
+ org.apache.hadoop.io.compress.BZip2Codec
+ org.apache.hadoop.io.compress.DeflateCodec
+ org.apache.hadoop.io.compress.SnappyCodec
+ org.apache.hadoop.io.compress.Lz4Codec
+ com.hadoop.compression.lzo.LzoCodec
+ com.hadoop.compression.lzo.LzopCodec

#### 原始数据压缩

##### 优化场景

推测表数据量过大，需要进行数据压缩

##### 优化原理

在建表阶段进行文件压缩设置，如果是数据源的话，采用bz或gz的方式，这样可以很大程度上节省磁盘空间；而在计算的过程中，为了不影响执行的速度，可以浪费一点磁盘空间，建议采用snappy的方式，这样可以整体提升hive的执行速度。 一般采用ORC文件和Parquet文件方式，ORC有默认的 ZLIB 压缩方式，可以选择Snappy方式压缩

> [参照文档]( https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC )

##### 优化方式

```sql
create table Addresses (
  name string,
  street string,
  city string,
  state string,
  zip int
) stored as orc tblproperties ("orc.compress"="NONE"); --NONE/SNAPPY/ZLIB
```

#### MR中间输出压缩

##### 优化原理

 对于中间数据压缩，选择一个低cpu开销的编/解码器要比选择一个压缩率高的编/解码器要重要的多。中间结果进行压缩，降低服务器之间的网络开销和数据。

##### 优化方式

1. 对中间结果压缩

   ```properties
   mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec/LzoCodec 中间结果压缩方法
   hive.exec.compress.intermediate=true
   ```

2. 对结果压缩

   ```properties
   hive.exec.compress.output=true;
   mapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec/SnappyCodec/DefaultCodec
   mapred.output.compression.type=BLOCK;NONE/RECORD(默认)/BLOCK
   ```

##### 优化结果

###### 中间结果压缩

+ 优化前

  ![1571278416347](C:\Users\23924\AppData\Roaming\Typora\typora-user-images\1571278416347.png)

  ![1571277579960](https://s2.ax1x.com/2019/10/17/KkhNUH.md.png)

+ 优化后

  ![1571278523206](C:\Users\23924\AppData\Roaming\Typora\typora-user-images\1571278523206.png)

###### 输出结果压缩

+ 优化前

  ![1571278924506](C:\Users\23924\AppData\Roaming\Typora\typora-user-images\1571278924506.png)

  ![1571279011829](C:\Users\23924\AppData\Roaming\Typora\typora-user-images\1571279011829.png)

+ 优化后

  ![1571279237957](C:\Users\23924\AppData\Roaming\Typora\typora-user-images\1571279237957.png)



#### 中间表格式选择

Hive存储方式对比

| 存储类型     | 是否可分         | 是否默认压缩 | 压缩格式 | 支持的压缩格式   | 存储结构                            | 优缺点                                                       |
| ------------ | ---------------- | ------------ | -------- | ---------------- | ----------------------------------- | ------------------------------------------------------------ |
| TextFile     | 是，压缩后不可分 | 否           |          | Gzip             | all                                 | 反序列化过程中，必须逐个字符判断是不是分隔符和行结束符，因此反序列化开销会比SequenceFile高几十倍 |
| SequenceFile | 是               | 是           | 默认压缩 | 已有压缩方式     | RECORD(默认)，选用block性能更为优秀 | 和hadoop api中的MapFile是相互兼容的                          |
| RCFile       | 是               | 是           | 无       | zlib,snappy,none | 列存储                              |                                                              |
| ORCFile      | 是               | 是           | zlib     | zlib,snappy,none | 行列存储                            | 性能优良，支持列查询，update/delete                          |
| Parquet      | 是               | 是           | Snappy   | Snappy、Gzip     | 列存储                              | 性能优良                                                     |

[Parquet、ORC、Text性能对比]( https://blog.csdn.net/yu616568/article/details/51868447 )

### JOB优化

#### 执行模式

##### 本地模式（小数据量）

- set hive.exec.mode.local.auto
- set hive.exec.mode.local.auto.input.files.max
- set hive.exec.mode.local.auto.inputbytes.max

##### 分布式模式

- 正常job

#### JOIN算法

针对所有join会有并行度设置

```properties
hive.exec.parallel;
hive.exec.parallel.thread.number;
```

##### common join

两个表数据量差不多，进行join操作，先按照block区分mapper，在reduce阶段，对所有的mapper进行关联操作，如果一个表的某个mapper数据量很大就会出现数据倾斜。

##### map join

```properties
set hive.auto.convert.join;
```

##### bucket map join

该情况是处理大表和大表进行关联的操作，将关联的表的列进行再次分桶，进行排序操作，通过`set hive.enforce.bucketing = true` 可以自动控制上一轮reduce的数量从而适配bucket的个数。

#### 数据倾斜

| **关键词**         | **情形**                                    | **后果**                                     |
| ------------------ | ------------------------------------------- | -------------------------------------------- |
| **Join**           | 其中一个表较小，但是key集中                 | 分发到某一个或几个Reduce上的数据远高于平均值 |
|                    | 大表与大表，但是分桶的判断字段0值或空值过多 | 这些空值都由一个reduce处理，灰常慢           |
| **group by**       | group by 维度过小，某值的数量过多           | 处理某值的reduce灰常耗时                     |
| **Count Distinct** | 某特殊值过多                                | 处理此特殊值的reduce耗时                     |

##### 解决方式

###### 参数调节

`hive.map.aggr=true`

Map 端部分聚合，相当于Combiner

`hive.groupby.skewindata=true`

有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。

###### 语句调节

+ 选用join key分布最均匀的表作为驱动表 
+ join前先做好过滤
+ count distinct 空值处理，业务上不需要提前过滤，业务上需要sum()...group by进行统计，如果group by维度较小，先做统计最后单独union

[join详解]( https://blog.csdn.net/qq_26442553/article/details/80865014 )

### SQL作业优化

#### 作业并行执行


## Spark程序优化(Scala)
## SQL和Spark SQL优化

####  用insert into替换union all 

##### 没有业务联系



### SQL 执行流程

| 执行流程           | 主要操作                             |
| ------------------ | ------------------------------------ |
| TableScanOperator  | 扫描数据表                           |
| ReduceSinkOperator | 创建将发送到Reduce端的<key,reduce>对 |
| JoinOperator       | Join两份数据                         |
| SelectOperator     | 选择输出列                           |
| FileSinkOperator   | 建立结果数据，输出至文件             |
| FilterOperator     | 过滤输入数据                         |
| GroupByOperator    | Group By语句                         |
| MapJoinOperator    | /*+mapjoin(t)*/                      |
| LimitOperator      | Limit语句处理                        |
| UnionOperator      | Union语句处理                        |

## Yarn调度优化
## HDFS优化